{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization in NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3a0O-SGoD1P"
      },
      "source": [
        "##NLTK Word Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sm_R_D5gOUx",
        "outputId": "0893aa92-5d97-4ee3-8a53-c9397c7a7487"
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import (word_tokenize,\n",
        "                          sent_tokenize,\n",
        "                          TreebankWordTokenizer,\n",
        "                          wordpunct_tokenize,\n",
        "                          TweetTokenizer,\n",
        "                          MWETokenizer)\n",
        "\n",
        "text=\"Hope, is the only thing stronger that fear! #Hope #Amal.M\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K53ov-rbnRKZ",
        "outputId": "77b4592c-5a3b-4573-8048-a71ca5c1dd71"
      },
      "source": [
        "#word Tokenizer\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'that', 'fear', '!', '#', 'Hope', '#', 'Amal.M']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOSZWvVEnbng",
        "outputId": "4e096606-4db4-4a74-b0e7-080565ae5629"
      },
      "source": [
        "#Sentence Tokenizer\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hope, is the only thing stronger that fear!', '#Hope #Amal.M']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0683Sg1LnnZo",
        "outputId": "f85b4088-67d1-4d0e-a968-a4c7b772b040"
      },
      "source": [
        "#Punctuation-based tokenizer\n",
        "print(wordpunct_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'that', 'fear', '!', '#', 'Hope', '#', 'Amal', '.', 'M']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFPlVeCpoXIm",
        "outputId": "372d499c-ff59-4ff7-c75c-e75971ecbe7d"
      },
      "source": [
        "#Treebank Word tokenizer\n",
        "text=\"What you don't want to be done to yourself, don't do to others...\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['What', 'you', 'do', \"n't\", 'want', 'to', 'be', 'done', 'to', 'yourself', ',', 'do', \"n't\", 'do', 'to', 'others', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4MKcIvIouo2",
        "outputId": "7ea905cd-659a-4237-a6b9-e80526fa4e39"
      },
      "source": [
        "#Tweet tokenizer\n",
        "tweet= \"Don't take cryptocurrency advice from people on Twitter ðŸ˜…ðŸ‘Œ\"\n",
        "tokenizer= TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Don't\", 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'Twitter', 'ðŸ˜…', 'ðŸ‘Œ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0aZtgmdpXJF",
        "outputId": "60313c50-272a-4ec9-9bb3-534173484e9c"
      },
      "source": [
        "#MWET tokenizer\n",
        "text= \"Hope, is the only thing stronger than fear! Hunger Games #Hope\"\n",
        "tokenizer= MWETokenizer()\n",
        "print(tokenizer.tokenize(word_tokenize(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger', 'Games', '#', 'Hope']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhRMUu_XqCGc"
      },
      "source": [
        "##TextBlob Word Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6VVNK-ipwqV"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QXWL6fpqJ58"
      },
      "source": [
        "text=\" But I'm glad you'll see me as I am. Above all, I wouldn't want people to think that I want to prove anything. I don't want to prove anything, I just want to live; to cause no evil to anyone but myself. I have that right, haven't I? Leo Tolstoy \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwk_xNwJqpgT",
        "outputId": "eae02000-8195-41d1-8edf-218d68be3c9c"
      },
      "source": [
        "blob_object= TextBlob(text)\n",
        "#Word tokenization of  the text\n",
        "text_words = blob_object.words\n",
        "#To see all tokens\n",
        "print(text_words)\n",
        "#To count the number of tokens\n",
        "print(len(text_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['But', 'I', \"'m\", 'glad', 'you', \"'ll\", 'see', 'me', 'as', 'I', 'am', 'Above', 'all', 'I', 'would', \"n't\", 'want', 'people', 'to', 'think', 'that', 'I', 'want', 'to', 'prove', 'anything', 'I', 'do', \"n't\", 'want', 'to', 'prove', 'anything', 'I', 'just', 'want', 'to', 'live', 'to', 'cause', 'no', 'evil', 'to', 'anyone', 'but', 'myself', 'I', 'have', 'that', 'right', 'have', \"n't\", 'I', 'Leo', 'Tolstoy']\n",
            "55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zeb6JyRrSfL"
      },
      "source": [
        "##spaCy Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28VoCczOrOLT",
        "outputId": "b3d77d98-1e4f-4345-c862-f3079139ceb0"
      },
      "source": [
        "!pip install spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSE2W9JArZk7",
        "outputId": "8707e682-93a2-427f-bb0f-233c77734cb7"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "text= \"All happy families are alike; each unhappy family is unhappy in its own way!!! ðŸ‘ŒðŸ‘Œ #Leo Tolstoy\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "  print(token, token.idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All 0\n",
            "happy 4\n",
            "families 10\n",
            "are 19\n",
            "alike 23\n",
            "; 28\n",
            "each 30\n",
            "unhappy 35\n",
            "family 43\n",
            "is 50\n",
            "unhappy 53\n",
            "in 61\n",
            "its 64\n",
            "own 68\n",
            "way 72\n",
            "! 75\n",
            "! 76\n",
            "! 77\n",
            "ðŸ‘Œ 79\n",
            "ðŸ‘Œ 80\n",
            "# 82\n",
            "Leo 83\n",
            "Tolstoy 87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xthA93cHsmDK"
      },
      "source": [
        "##Gensim Word tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x35SocqZr6HK",
        "outputId": "b58b373b-a487-4ac8-ad07-24bee86a17ec"
      },
      "source": [
        "from gensim.utils import tokenize \n",
        "list(tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['All',\n",
              " 'happy',\n",
              " 'families',\n",
              " 'are',\n",
              " 'alike',\n",
              " 'each',\n",
              " 'unhappy',\n",
              " 'family',\n",
              " 'is',\n",
              " 'unhappy',\n",
              " 'in',\n",
              " 'its',\n",
              " 'own',\n",
              " 'way',\n",
              " 'Leo',\n",
              " 'Tolstoy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvcEDRXutH5g"
      },
      "source": [
        "##Tokenization with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPYxbau5s-Ew"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "151FXXIutXL2"
      },
      "source": [
        "ntoken = Tokenizer(num_words=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpF4-cVxtbn8",
        "outputId": "2246acbd-5217-4288-8f8b-32c910cca240"
      },
      "source": [
        "ntoken.fit_on_texts(text)\n",
        "list_words = text_to_word_sequence(text)\n",
        "print(list_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['all', 'happy', 'families', 'are', 'alike', 'each', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', 'ðŸ‘ŒðŸ‘Œ', 'leo', 'tolstoy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnMgiVJRtowZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}